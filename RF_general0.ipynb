{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import os\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import tensorflow as tf\n",
        "from contextlib import redirect_stdout\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow_decision_forests as forest\n",
        "from tensorflow.python.client import device_lib"
      ],
      "metadata": {
        "id": "zjHNvOayxy0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting environment variables for TensorFlow configuration\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ],
      "metadata": {
        "id": "8CCa022F41QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting display options for pandas\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)"
      ],
      "metadata": {
        "id": "e3z5sOwQ41qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking available devices (useful for confirming GPU usage)\n",
        "print(device_lib.list_local_devices())"
      ],
      "metadata": {
        "id": "NqQWlbCq4_U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to plot variable importances for TensorFlow Decision Forests\n",
        "def plot_tfdf_importances(inspector: forest.inspector.AbstractInspector, importance_type: str, name: str):\n",
        "    scoredate = pd.DataFrame(columns=['name', 'scores'])\n",
        "    try:\n",
        "        importances = inspector.variable_importances()[importance_type]\n",
        "    except KeyError:\n",
        "        raise ValueError(f\"Unknown importance type: {importance_type}\")\n",
        "\n",
        "    # Setting the labels for the variable importance plot\n",
        "    plt.xlabel(importance_type)\n",
        "    plt.title(\"Variable Importance\")\n",
        "    scoredate['name'] = names\n",
        "    scoredate['scores'] = scores\n",
        "\n",
        "    # Saving the variable importance data and plot\n",
        "    scoredate.to_csv(name.replace('.png', '.csv'), index=False)\n",
        "    plt.savefig(name)\n",
        "    plt.show()\n",
        "\n",
        "# Configuring the task type for the decision forest\n",
        "task = forest.keras.Task.REGRESSION"
      ],
      "metadata": {
        "id": "ksJlSv_sxzCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asking the user for the resolution of the dataset\n",
        "resolution = [input(\"Enter the resolution of the dataset: \")]\n",
        "\n",
        "# Constructing paths based on zone and resolution\n",
        "home_directory = input(\"Enter the home directory where the datasets are located: \")"
      ],
      "metadata": {
        "id": "3s6mGwmYxzL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asking for the name of the training dataset and constructing the full path\n",
        "train_dataset_name = input(\"Enter the name of the training dataset (including file extension): \")\n",
        "train_file = os.path.join(home_directory, train_dataset_name)\n",
        "\n",
        "# Checking if the training file exists and prompting the user until a valid file is provided\n",
        "while not os.path.exists(train_file):\n",
        "    print(\"The specified training file does not exist. Please enter the correct dataset name.\")\n",
        "    train_dataset_name = input(\"Enter the name of the training dataset (including file extension): \")\n",
        "    train_file = os.path.join(home_directory, train_dataset_name)"
      ],
      "metadata": {
        "id": "ZVsEuQ1GxzVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asking for the name of the testing dataset and constructing the full path\n",
        "test_dataset_name = input(\"Enter the name of the testing dataset (including file extension): \")\n",
        "test_file = os.path.join(home_directory, test_dataset_name)\n",
        "\n",
        "# Checking if the testing file exists and prompting the user until a valid file is provided\n",
        "while not os.path.exists(test_file):\n",
        "    print(\"The specified testing file does not exist. Please enter the correct dataset name.\")\n",
        "    test_dataset_name = input(\"Enter the name of the testing dataset (including file extension): \")\n",
        "    test_file = os.path.join(home_directory, test_dataset_name)"
      ],
      "metadata": {
        "id": "bA0j18oJyNh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining columns for training, testing, and exporting data\n",
        "var1 = ['Clay', 'Sand', 'Silt', 'Elevation', 'Aspect', 'Slope', 'Lai', 'SMERGE', 'NDVI', 'Albedo', 'Temp',\"Date\"]  # training vars\n",
        "var2 = ['Clay', 'Sand', 'Silt', 'Elevation', 'Aspect', 'Slope', 'Lai', 'NDVI', 'Albedo', 'Temp',\"Date\"]  # testing vars\n",
        "var3 = ['Clay', 'Sand', 'Silt', 'Elevation', 'Aspect', 'Slope', 'Lai', 'SMERGE', 'NDVI', 'Albedo', 'Temp',\"Date\"]  # vars for export csv"
      ],
      "metadata": {
        "id": "yZ7D1rHJyNsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading training data\n",
        "train_data = pd.read_csv(train_file,\n",
        "                         usecols=['SMERGE', 'Date', 'Temp', 'PageName', 'LAI', 'Albedo', 'NDVI', 'Clay', 'Sand', 'Silt',\n",
        "                                  'Slope', 'Elevation', 'Ascept'])\n",
        "train_data = train_data[\n",
        "    ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept', 'Slope', 'NDVI', 'SMERGE', 'Date', 'LAI', 'Albedo', 'Temp']]\n",
        "print(train_data)"
      ],
      "metadata": {
        "id": "zh1vVPQ5yN0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading test data\n",
        "test_data = pd.read_csv(test_file,\n",
        "                        usecols=['SMERGE', 'Date', 'Temp', 'PageName', 'LAI', 'Albedo', 'NDVI', 'Clay', 'Sand', 'Silt',\n",
        "                                 'Slope', 'Elevation', 'Ascept'])\n",
        "test_page = test_data[['PageName']]\n",
        "ndvi = test_data['NDVI']\n",
        "dates = test_data['Date']\n",
        "test_data.drop(['PageName'], axis=1)\n",
        "test_data = test_data[\n",
        "    ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept', 'Slope', 'NDVI', 'SMERGE', 'Date', 'LAI', 'Albedo', 'Temp']]\n",
        "print(test_data)\n",
        "\n",
        "# Defining columns for scaling\n",
        "scale_columns = ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept', 'Slope', 'NDVI', 'SMERGE']"
      ],
      "metadata": {
        "id": "H6g23CKpyN5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for the model training\n",
        "# 't': affects the number of trees,\n",
        "# 'k': number of trials in random search\n",
        "t = 1100\n",
        "k = 15\n",
        "tuner = forest.tuner.RandomSearch(num_trials=9 * k, use_predefined_hps=True)\n",
        "model_img_file = resolution + \"model.png\"\n",
        "n = test_data.shape[0]\n",
        "\n",
        "# Initializing output array\n",
        "out = np.empty(shape=(0, 1))\n",
        "\n",
        "# Creating an empty DataFrame with specific columns\n",
        "x = pd.DataFrame(columns=var3)"
      ],
      "metadata": {
        "id": "gLiKpZkmyN-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring the random forest model\n",
        "model = forest.keras.RandomForestModel(\n",
        "        verbose=1, tuner=tuner, num_trees=t, allow_na_conditions=True, task=task, winner_take_all=True,\n",
        "        categorical_algorithm='CART', honest=True, honest_fixed_separation=True,\n",
        "        honest_ratio_leaf_examples=0.75, bootstrap_size_ratio=1.05,\n",
        "        adapt_bootstrap_size_ratio_for_maximum_training_duration=True,\n",
        "        keep_non_leaf_label_distribution=False, num_threads=12, max_depth=9)"
      ],
      "metadata": {
        "id": "0ArNWTRuyOHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing 'Date' column for training\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'], format=\"%m/%d/%Y\").astype(int)\n",
        "\n",
        "# Creating a TensorFlow dataset for training\n",
        "train_ds = forest.keras.pd_dataframe_to_tf_dataset(train_data[var1], label='SMERGE', task=task)\n",
        "\n",
        "# Compiling and training the model\n",
        "model.compile(metrics=[\"mae\"])\n",
        "model.fit(train_ds)"
      ],
      "metadata": {
        "id": "3Bk0633qzNGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a TensorFlow dataset for testing\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'], format=\"%m/%d/%Y\").astype(int)\n",
        "test_data1 = test_data\n",
        "test_ds = forest.keras.pd_dataframe_to_tf_dataset(test_data1[var2], task=task)\n",
        "\n",
        "# Extracting relevant data for output processing\n",
        "out_data = test_data1[var3]"
      ],
      "metadata": {
        "id": "HgalphUFzNLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting with the trained model\n",
        "pred = model.predict(test_ds, verbose=1)\n",
        "out = np.vstack([out, pred])\n",
        "x = pd.concat([x, out_data])"
      ],
      "metadata": {
        "id": "oiENzgCmzNQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the model summary to a text file\n",
        "with open(resolution + \"sum_model.txt\", \"w\") as txt_file:\n",
        "    with redirect_stdout(txt_file):\n",
        "        model.summary()"
      ],
      "metadata": {
        "id": "V_GhLP9hzNVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model plot to an HTML file\n",
        "with open(resolution + \"model.html\", \"w\") as html_file:\n",
        "    html_file.write(forest.model_plotter.plot_model(model, tree_idx=0, max_depth=10))"
      ],
      "metadata": {
        "id": "KMffcgmxzNaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resetting the model states (if any)\n",
        "model.reset_states()"
      ],
      "metadata": {
        "id": "kXhFsoI8z_aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the output dataframe\n",
        "x['ML_'] = out\n",
        "df_out = pd.DataFrame(pred, columns=['ML_'])\n",
        "x['NDVI'] = ndvi\n",
        "x['Date'] = dates\n",
        "x['PageName'] = test_page\n",
        "\n",
        "# Saving the output dataframe to a CSV file\n",
        "x.to_csv(resolution + \".csv\", index=False)"
      ],
      "metadata": {
        "id": "PVO-P9XFz_lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating variable importance plots\n",
        "inspector = model.make_inspector()\n",
        "plot_tfdf_importances(inspector=inspector, importance_type='INV_MEAN_MIN_DEPTH', name=resolution + \"_IMMD.png\")\n",
        "plot_tfdf_importances(inspector=inspector, importance_type='NUM_NODES', name=resolution + \"_NumNodes.png\")"
      ],
      "metadata": {
        "id": "GflQHWVTz_r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running another script at the end of the current script\n",
        "subprocess.run([\"python\", \"Era3_1400.py\"])"
      ],
      "metadata": {
        "id": "yyfReNLv0RSI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}