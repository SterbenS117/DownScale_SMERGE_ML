{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s15Z_vIVKIyO"
      },
      "outputs": [],
      "source": [
        "# Install SHAP if not already in library\n",
        "#pip install shap\n",
        "# Install os\n",
        "#pip install os\n",
        "\n",
        "# Importing the necessary libraries\n",
        "import os\n",
        "import io\n",
        "import shap\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import mean, std\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets, ensemble\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "# Asking the user for the resolution of the dataset\n",
        "resolution = input(\"Enter the resolution of the dataset: \")\n",
        "## e.g. Enter the resolution of the dataset: 1000m\n",
        "\n",
        "# Constructing paths based on the zone and resolution\n",
        "home_directory = input(\"Enter the home directory where the datasets are located: \")\n",
        "## e.g. Enter the home directory where the datasets are located: /content/ARM_1/\n",
        "\n",
        "# Asking for the name of the testing dataset and constructing the full path\n",
        "test_dataset_name = input(\"Enter the name of the testing dataset (including file extensions): \")\n",
        "## e.g. Enter the name of the testing dataset: ERA1_2ARM_1000m_dataV1_test.csv\n",
        "\n",
        "test_file = os.path.join(home_directory, test_dataset_name)\n",
        "\n",
        "# Checking if the testing file exists and prompting the user until a valid file is provided\n",
        "while not os.path.exists(test_file):\n",
        "  print(\"The specified training file does not exist. Enter the correct dataset name.\")\n",
        "  test_dataset_name = input(\"Enter the name of the testing dataset (including file extensions): \")\n",
        "  test_file = os.path.join(home_directory, test_dataset_name)\n",
        "\n",
        "# Asking for the name of the training dataset and constructing the full path\n",
        "train_dataset_name = input(\"Enter the name of the training dataset (including file extensions): \")\n",
        "## e.g. Enter the name of the training dataset: ERA1_2ARM_1000m_dataV1_train.csv\n",
        "\n",
        "train_file = os.path.join(home_directory, train_dataset_name)\n",
        "\n",
        "# Checking if the training file exists and prompting the user until a valid file is provided\n",
        "while not os.path.exists(train_file):\n",
        "  print(\"The specified training file does now exist. Enter the correct dataset name.\")\n",
        "  train_dataset_name = input(\"Enter the name of the training dataset (inlcluding file extensions): \")\n",
        "  train_file = os.path.join(home_directory, train_dataset_name)\n",
        "\n",
        "# Reading the test data\n",
        "test_data = pd.read_csv(test_file, usecols=['SMERGE', 'Date','PageName', 'LAI',\n",
        "                                         'Albedo', 'NDVI', 'Clay', 'Sand',\n",
        "                                         'Silt', 'Slope', 'Elevation', 'Ascept', 'Temp'])\n",
        "test_data = test_data.loc[:, ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept',\n",
        "                                      'Slope', 'NDVI', 'SMERGE', 'Date', 'LAI', 'Albedo', 'Temp']]\n",
        "test_data.columns = ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept', 'Slope',\n",
        "                             'NDVI', 'SMERGE', 'Date', 'Lai', 'Albedo', 'Temp']\n",
        "\n",
        "# Reading the training data\n",
        "train_data = pd.read_csv(train_file, usecols=['SMERGE', 'Date', 'PageName', 'LAI',\n",
        "                                              'Albedo', 'NDVI', 'Clay', 'Sand', 'Silt',\n",
        "                                          'Slope', 'Elevation', 'Ascept', 'Temp'])\n",
        "train_data = train_data.loc[:, ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept',\n",
        "                                'Slope', 'NDVI', 'SMERGE', 'Date', 'LAI', 'Albedo', 'Temp']]\n",
        "train_data.columns = ['Clay', 'Sand', 'Silt', 'Elevation', 'Slope', 'Ascept',\n",
        "                      'NDVI', 'SMERGE', 'Date', 'Lai', 'Albedo', 'Temp']\n",
        "\n",
        "# Reading the testing files for: 'PageName' and 'Date'\n",
        "page = pd.read_csv(test_file, usecols=['PageName'])\n",
        "date = pd.read_csv(test_file, usecols=['Date'])\n",
        "\n",
        "# Processing 'Date' column for testing\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'], format=\"%m/%d/%Y\").astype(int)\n",
        "\n",
        "# Processing 'Date' column for training\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'], format=\"%m/%d/%Y\").astype(int)\n",
        "\n",
        "# Separating target variable in variables\n",
        "y_test = test_data[['SMERGE']]\n",
        "x_test = test_data[\n",
        "    ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept', 'Slope', 'Lai', 'NDVI', 'Albedo', \"Temp\", \"Date\"]]\n",
        "y_train = train_data[['SMERGE']]\n",
        "x_train = train_data[\n",
        "    ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept', 'Slope', 'Lai', 'NDVI', 'Albedo', \"Temp\", \"Date\"]]\n",
        "\n",
        "# Stating the parameters which will be used for GBR\n",
        "params = {\n",
        "    \"n_estimators\": 200,\n",
        "    \"max_depth\": 10,\n",
        "    \"min_samples_split\": 4,\n",
        "    \"learning_rate\": 0.3,\n",
        "    \"loss\": \"squared_error\",\n",
        "    \"verbose\": 1\n",
        "}\n",
        "\n",
        "# Converting to numpy array\n",
        "x_train_nu = x_train.to_numpy()\n",
        "y_train_nu = y_train.to_numpy()\n",
        "x_test_nu = x_test.to_numpy()\n",
        "\n",
        "    ### GRADIENT BOOSTING REGRESSION\n",
        "# Configuring the Gradient Boosting Regressor\n",
        "reg = ensemble.GradientBoostingRegressor(**params)\n",
        "reg.fit(x_train, y_train.values.ravel())\n",
        "\n",
        "mse = mean_squared_error(y_test, reg.predict(x_test))\n",
        "print(\"The mean squared error (MSE) on the set: {:.4f}\".format(mse))\n",
        "\n",
        "# Predicting the test model to prepare for the output dataframe\n",
        "h = reg.predict(x_test_nu)\n",
        "test_data['Date'] = date['Date']\n",
        "test_data['ML'] = h\n",
        "test_data['PageName'] = page\n",
        "\n",
        "# Saving the output dataframe to a CSV file\n",
        "test_data.to_csv(resolution + \".csv\", index=False)\n",
        "\n",
        "## Using SHAP\n",
        "# Generating variable importance plots\n",
        "my_model = reg.fit(x_train, y_train)\n",
        "X_sampled = x_train.sample(1000, random_state=10, replace=True)\n",
        "\n",
        "explainer = shap.Explainer(my_model, X_sampled)\n",
        "shap_values = explainer(X_sampled)\n",
        "shap.plots.bar(shap_values, max_display=11)\n",
        "\n",
        "# Saving variable importance plot\n",
        "mean_shap = np.abs(shap_values.values).mean(axis=0)\n",
        "shap_pd = pd.DataFrame(mean_shap, index=X_sampled.columns).sort_values(by=[0], ascending=False)\n",
        "shap_pd.to_csv(resolution + '_SHAP.csv')"
      ]
    }
  ]
}