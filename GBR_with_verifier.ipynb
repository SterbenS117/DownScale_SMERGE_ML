{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCuW4uGlGab6"
      },
      "outputs": [],
      "source": [
        "# Install SHAP if not already in library\n",
        "#  pip install shap\n",
        "# Install os\n",
        "# pip install os\n",
        "\n",
        "# Importing the necessary libraries\n",
        "import os\n",
        "import io\n",
        "import shap\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets, ensemble\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "\n",
        "# Asking the user for the resolution of the dataset\n",
        "resolution = input(\"Enter the resolution of the dataset: \")\n",
        "## e.g. Enter the resolution of the dataset:  400m\n",
        "\n",
        "# Constructing paths based on zome and resolution\n",
        "home_directory = input(\"Enter the home directory where the datasets are located: \")\n",
        "## e.g. Enter the home directory where the datasets are located:  /content/airmoss/\n",
        "\n",
        "# Asking for the name of the testing dataset and constructing the full path\n",
        "test_dataset_name = input(\"Enter the name of the testing dataset (including file extensions): \")\n",
        "## e.g. Enter the name of the testing dataset: A4_400m_processed__test.csv\n",
        "\n",
        "test_file = os.path.join(home_directory, test_dataset_name)\n",
        "\n",
        "# Checking if the testing file exists and prompting the user until a valid file is provided\n",
        "while not os.path.exists(test_file):\n",
        "  print(\"The specified training file does not exist. Enter the correct dataset name.\")\n",
        "  test_dataset_name = input(\"Enter the name of the testing dataset (including file extensions): \")\n",
        "  test_file = os.path.join(home_directory, test_dataset_name)\n",
        "\n",
        "# Asking for the name of the training dataset and constructing the full path\n",
        "train_dataset_name = input(\"Enter the name of the training dataset (including file extensions): \")\n",
        "## e.g. Enter the name of the training dataset:  A4_400m_processed__train.csv\n",
        "train_file = os.path.join(home_directory, train_dataset_name)\n",
        "\n",
        "# Checking if the training file exists and prompting the user until a valid file is provided\n",
        "while not os.path.exists(train_file):\n",
        "  print(\"The specified training file does not exist. Enter the correct dataset name.\")\n",
        "  train_dataset_name = input(\"Enter the name of the training dataset (including file extensions): \")\n",
        "  train_file = os.path.join(home_directory, train_dataset_name)\n",
        "\n",
        "# Reading the test data\n",
        "test_data=pd.read_csv(test_file,\n",
        "                      usecols=[\"PageName\",\"Clay\",\"Sand\", 'Silt', \"Elevation\", \"Slope\", \"Ascept\", \"NDVI\", \"SMERGE\", \"Verifier\", \"Date\", \"LAI\", \"Albedo\",\n",
        "                                     'Temp'])\n",
        "\n",
        "test_data = test_data.loc[:, ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept',\n",
        "                              'Slope', 'NDVI', 'SMERGE', 'Date', 'LAI', 'Albedo', 'Temp']]\n",
        "\n",
        "# Reading the testing files for: 'PageName', 'Date', and 'Verifier'\n",
        "page = pd.read_csv(test_file, usecols=['PageName'])\n",
        "date = pd.read_csv(test_file, usecols=['Date'])\n",
        "ver = pd.read_csv(test_file, usecols=['Verifier'])\n",
        "\n",
        "# Retrieving the columns of the test data\n",
        "test_data.columns = ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept', 'Slope',\n",
        "                     'NDVI', 'SMERGE', 'Date', 'Lai', 'Albedo', ('%s' % 'Temp')]\n",
        "\n",
        "# Reading the training data\n",
        "train_data=pd.read_csv(train_file,\n",
        "                             usecols=[\"PageName\",\"Clay\",\"Sand\", 'Silt', \"Elevation\", \"Slope\", \"Ascept\", \"NDVI\", \"SMERGE\", \"Verifier\", \"Date\", \"LAI\", \"Albedo\",\n",
        "                                      'Temp'])\n",
        "\n",
        "train_data = train_data.loc[:, ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept',\n",
        "                                    'Slope', 'NDVI', 'SMERGE', 'Date', 'LAI', 'Albedo', 'Temp']]\n",
        "\n",
        "# Retrieving the columns of the training data\n",
        "train_data.columns = ['Clay', 'Sand', 'Silt', 'Elevation', 'Slope', 'Ascept',\n",
        "                          'NDVI', 'SMERGE', 'Date', 'Lai', 'Albedo', 'Temp']\n",
        "\n",
        "# Converting string column to datetime column\n",
        "# Processing 'Date' column for testing\n",
        "print(test_data['Date'])\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'], format=\"%Y/%m/%d\").astype(int)\n",
        "\n",
        "# Processing 'Date' column for training\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'], format=\"%Y/%m/%d\").astype(int)\n",
        "\n",
        "##########\n",
        "# Separating target variable in variables\n",
        "y_test = test_data[['SMERGE']]\n",
        "x_test = test_data[\n",
        "        ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept', 'Slope', 'Lai', 'NDVI', 'Albedo', 'Temp', \"Date\"]]\n",
        "y_train = train_data[['SMERGE']]\n",
        "x_train = train_data[\n",
        "        ['Clay', 'Sand', 'Silt', 'Elevation', 'Ascept', 'Slope', 'Lai', 'NDVI', 'Albedo', 'Temp', \"Date\"]]\n",
        "\n",
        "# Stating the parameters which will be used for GBR\n",
        "params = {\n",
        "    \"n_estimators\": 200,\n",
        "    \"max_depth\": 10,\n",
        "    \"min_samples_split\": 4,\n",
        "    \"learning_rate\": 0.3,\n",
        "    \"loss\": \"squared_error\",\n",
        "    \"verbose\": 1\n",
        "}\n",
        "\n",
        "# Converting to numpy array\n",
        "x_train_nu = x_train.to_numpy()\n",
        "y_train_nu = y_train.to_numpy()\n",
        "x_test_nu = x_test.to_numpy()\n",
        "\n",
        "#### GRADIENT BOOSTING REGRESSION\n",
        "# Configuring the Gradient Boosting Regressor\n",
        "reg = ensemble.GradientBoostingRegressor(**params)\n",
        "reg.fit(x_train, y_train.values.ravel())\n",
        "\n",
        "mse = mean_squared_error(y_test, reg.predict(x_test))\n",
        "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
        "\n",
        "# Predicting the test model to prepare for the output dataframe\n",
        "h = reg.predict(x_test_nu)\n",
        "test_data['Date'] = date['Date']\n",
        "test_data['ML_'] = h\n",
        "test_data['Verifier'] = ver\n",
        "test_data['PageName'] = page\n",
        "\n",
        "# Saving the output dataframe to a CSV file\n",
        "test_data.to_csv(resolution + \".csv\", index=False)\n",
        "\n",
        "# Generating variable importance plots\n",
        "model = reg.fit(x_train, y_train)\n",
        "X_sampled = x_train.sample(1000, random_state=10)\n",
        "\n",
        "explainer = shap.Explainer(model, X_sampled)\n",
        "shap_values = explainer(X_sampled, check_additivity=False)\n",
        "shap.plots.bar(shap_values, max_display=11)\n",
        "\n",
        "# Saving variable importance plot\n",
        "mean_shap = np.abs(shap_values.values).mean(axis=0)\n",
        "shap_pd = pd.DataFrame(mean_shap, index=X_sampled.columns).sort_values(by=[0], ascending=False)\n",
        "shap_pd.to_csv(resolution[0] + '_SHAP.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPEN RF_DOC_SAMPLE.PY"
      ],
      "metadata": {
        "id": "QCl3Wmw85oxq"
      }
    }
  ]
}